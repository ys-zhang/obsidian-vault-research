% Encoding: UTF-8

@Article{vaandrager2017model,
  author     = {Vaandrager, Frits},
  journal    = {Communications of the ACM},
  title      = {Model learning},
  year       = {2017},
  number     = {2},
  pages      = {86--95},
  volume     = {60},
  abstract   = {Model learning emerges as an effective method for black-box state machine models of hardware and software components.},
  file       = {:vaandrager2017model.pdf:PDF},
  groups     = {protocol state fuzzing},
  keywords   = {model learning, model checking, protocol state learning},
  publisher  = {ACM New York, NY, USA},
  readstatus = {skimmed},
  url        = {https://dl.acm.org/doi/fullHtml/10.1145/2967606},
}

@Article{YE2021195,
  author   = {Aoshuang Ye and Lina Wang and Lei Zhao and Jianpeng Ke and Wenqi Wang and Qinliang Liu},
  journal  = {Neurocomputing},
  title    = {RapidFuzz: Accelerating fuzzing via Generative Adversarial Networks},
  year     = {2021},
  issn     = {0925-2312},
  pages    = {195-204},
  volume   = {460},
  abstract = {We implement a Generative Adversarial Network (GAN) based fuzzer called RapidFuzz to generate synthetic testcase, which can precisely catch the data structure feature in a relatively shorter time than the state-of-art fuzzers. RapidFuzz provides potential seeds generated by GAN. i.e., The generated seeds with similar but different numerical distributions accelerate the mutation process. An algorithm is elaborately designed to locate the hot-points generated by GAN. The generated testcases make structural features easier to be identified, which makes the whole process faster. In our experiment, RapidFuzz considerably improves the performance of American Fuzzy Lop(AFL) in speed, coverage, and mapsize. We select 9 open-sourced programs with different highly-structured inputs to demonstrate the effectiveness of RapidFuzz. As a result, code coverage is significantly improved. For tiff2pdf and tiffdump, coverage increase exceeds over 20%. We also observe that RapidFuzz achieves the same coverage with less time than AFL. Furthermore, AFL absorbs 21% of generated seed files in tiff2pdf with an average absorption rate around 15% in other programs.},
  comment  = {# IDEA  

GAN generation can be adopted to accelerate the mutation speed, which allows our method to skip part of meaningless mutation.

GAN has a fame for generating images based on the observation that pixels change
smoothly. 

A probability density matrix seems reasonable to describe the status of the image.},
  doi      = {https://doi.org/10.1016/j.neucom.2021.06.082},
  file     = {:YE2021195.pdf:PDF},
  groups   = {machine learning},
  keywords = {AFL, Fuzzing, Generative Adversarial Networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231221010122},
}

@Article{bohme2017coverage,
  author    = {B{\"o}hme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
  journal   = {IEEE Transactions on Software Engineering},
  title     = {Coverage-based greybox fuzzing as markov chain},
  year      = {2017},
  number    = {5},
  pages     = {489--506},
  volume    = {45},
  file      = {:bohme2017coverage.pdf:PDF;note:C\:/Users/ys-zh/iCloudDrive/iCloud~md~obsidian/ReSch/Testing/papers/Coverage-Based Greybox Fuzzing as Markov Chain.md:},
  publisher = {IEEE},
}

@InProceedings{garhewal2020grey,
  author       = {Garhewal, Bharat and Vaandrager, Frits and Howar, Falk and Schrijvers, Timo and Lenaerts, Toon and Smits, Rob},
  booktitle    = {International Conference on Integrated Formal Methods},
  title        = {Grey-box learning of register automata},
  year         = {2020},
  organization = {Springer},
  pages        = {22--40},
  file         = {:garhewal2020grey.pdf:PDF},
  groups       = {protocol state fuzzing},
}

@InProceedings{aschermann2019redqueen,
  author    = {Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
  booktitle = {NDSS},
  title     = {REDQUEEN: Fuzzing with Input-to-State Correspondence.},
  year      = {2019},
  pages     = {1--15},
  volume    = {19},
  comment   = {[conference ]https://youtu.be/9JpanJ29r_U},
  file      = {:NDSS19-Redqueen.pdf:PDF},
  url       = {https://www.ndss-symposium.org/ndss-paper/redqueen-fuzzing-with-input-to-state-correspondence/},
}

@InProceedings{274634,
  author    = {Andrea Fioraldi and Daniele Cono D{\textquoteright}Elia and Davide Balzarotti},
  booktitle = {30th {USENIX} Security Symposium ({USENIX} Security 21)},
  title     = {The Use of Likely Invariants as Feedback for Fuzzers},
  year      = {2021},
  month     = aug,
  pages     = {2829--2846},
  publisher = {{USENIX} Association},
  file      = {:usenixsec21_fioraldi.pdf:PDF},
  groups    = {program analysis},
  isbn      = {978-1-939133-24-3},
  url       = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
}

@Article{ERNST200735,
  author   = {Michael D. Ernst and Jeff H. Perkins and Philip J. Guo and Stephen McCamant and Carlos Pacheco and Matthew S. Tschantz and Chen Xiao},
  journal  = {Science of Computer Programming},
  title    = {The Daikon system for dynamic detection of likely invariants},
  year     = {2007},
  issn     = {0167-6423},
  note     = {Special issue on Experimental Software and Toolkits},
  number   = {1},
  pages    = {35-45},
  volume   = {69},
  abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon’s output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
  doi      = {https://doi.org/10.1016/j.scico.2007.01.015},
  groups   = {program analysis},
  keywords = {Daikon, Dynamic analysis, Dynamic invariant detection, Inductive logic programming, Inference, Invariant, Likely invariant, Program understanding, Specification, Specification mining},
  url      = {https://www.sciencedirect.com/science/article/pii/S016764230700161X},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:protocol state fuzzing\;0\;1\;0x008080ff\;\;\;;
1 StaticGroup:machine learning\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:program analysis\;0\;1\;0xffff66ff\;\;\;;
}
