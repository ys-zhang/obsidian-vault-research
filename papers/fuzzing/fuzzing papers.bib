% Encoding: UTF-8

@Article{vaandrager2017model,
  author     = {Vaandrager, Frits},
  journal    = {Communications of the ACM},
  title      = {Model learning},
  year       = {2017},
  number     = {2},
  pages      = {86--95},
  volume     = {60},
  abstract   = {Model learning emerges as an effective method for black-box state machine models of hardware and software components.},
  file       = {:vaandrager2017model.pdf:PDF},
  groups     = {protocol state fuzzing},
  keywords   = {model learning, model checking, protocol state learning},
  publisher  = {ACM New York, NY, USA},
  readstatus = {skimmed},
  url        = {https://dl.acm.org/doi/fullHtml/10.1145/2967606},
}

@Article{YE2021195,
  author   = {Aoshuang Ye and Lina Wang and Lei Zhao and Jianpeng Ke and Wenqi Wang and Qinliang Liu},
  journal  = {Neurocomputing},
  title    = {RapidFuzz: Accelerating fuzzing via Generative Adversarial Networks},
  year     = {2021},
  issn     = {0925-2312},
  pages    = {195-204},
  volume   = {460},
  abstract = {We implement a Generative Adversarial Network (GAN) based fuzzer called RapidFuzz to generate synthetic testcase, which can precisely catch the data structure feature in a relatively shorter time than the state-of-art fuzzers. RapidFuzz provides potential seeds generated by GAN. i.e., The generated seeds with similar but different numerical distributions accelerate the mutation process. An algorithm is elaborately designed to locate the hot-points generated by GAN. The generated testcases make structural features easier to be identified, which makes the whole process faster. In our experiment, RapidFuzz considerably improves the performance of American Fuzzy Lop(AFL) in speed, coverage, and mapsize. We select 9 open-sourced programs with different highly-structured inputs to demonstrate the effectiveness of RapidFuzz. As a result, code coverage is significantly improved. For tiff2pdf and tiffdump, coverage increase exceeds over 20%. We also observe that RapidFuzz achieves the same coverage with less time than AFL. Furthermore, AFL absorbs 21% of generated seed files in tiff2pdf with an average absorption rate around 15% in other programs.},
  comment  = {# IDEA  

GAN generation can be adopted to accelerate the mutation speed, which allows our method to skip part of meaningless mutation.

GAN has a fame for generating images based on the observation that pixels change
smoothly. 

A probability density matrix seems reasonable to describe the status of the image.},
  doi      = {https://doi.org/10.1016/j.neucom.2021.06.082},
  file     = {:YE2021195.pdf:PDF},
  groups   = {machine learning},
  keywords = {AFL, Fuzzing, Generative Adversarial Networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231221010122},
}

@Article{bohme2017coverage,
  author    = {B{\"o}hme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
  journal   = {IEEE Transactions on Software Engineering},
  title     = {Coverage-based greybox fuzzing as markov chain},
  year      = {2017},
  number    = {5},
  pages     = {489--506},
  volume    = {45},
  file      = {:bohme2017coverage.pdf:PDF;note:C\:/Users/ys-zh/iCloudDrive/iCloud~md~obsidian/ReSch/Testing/papers/Coverage-Based Greybox Fuzzing as Markov Chain.md:},
  publisher = {IEEE},
}

@InProceedings{garhewal2020grey,
  author       = {Garhewal, Bharat and Vaandrager, Frits and Howar, Falk and Schrijvers, Timo and Lenaerts, Toon and Smits, Rob},
  booktitle    = {International Conference on Integrated Formal Methods},
  title        = {Grey-box learning of register automata},
  year         = {2020},
  organization = {Springer},
  pages        = {22--40},
  file         = {:garhewal2020grey.pdf:PDF},
  groups       = {protocol state fuzzing},
}

@InProceedings{aschermann2019redqueen,
  author    = {Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
  booktitle = {NDSS},
  title     = {REDQUEEN: Fuzzing with Input-to-State Correspondence.},
  year      = {2019},
  pages     = {1--15},
  volume    = {19},
  abstract  = {Automated software testing based on fuzzing has experienced a revival in recent years. Especially feedback-driven fuzzing has become well-known for its ability to efficiently perform randomized testing with limited input corpora. 

Despite a lot of progress, two common problems are magic numbers and (nested) checksums. 

Computationally expensive methods such as taint tracking and symbolic execution are typically used to overcome such roadblocks. Unfortunately, such methods often require access to source code, a rather precise description of the environment (e.g., behavior of library calls or the underlying OS), or the exact semantics of the platform’s instruction set.

In this paper, we introduce a lightweight, yet very effective alternative to taint tracking and symbolic execution to facilitate and optimize state-of-the-art feedback fuzzing that easily scales to large binary applications and unknown environments. 

We observe that during the execution of a given program, parts of the input often end up directly (i.e., nearly unmodified) in the program state. 

This input-to-state correspondence can be exploited to create a robust method to overcome common fuzzing roadblocks in a highly effective and efficient manner. Our prototype implementation, called REDQUEEN, is able to solve magic bytes and (nested) checksum tests automatically for a given binary executable. Additionally, we show that our techniques outperform various state-of-the-art tools on a wide variety of targets across different privilege levels (kernel-space and userland) with no platform-specific code.

REDQUEEN is the first method to find more than 100% of the bugs planted in LAVA-M across all targets. Furthermore, we were able to discover 65 new bugs and obtained 16 CVEs in multiple programs and OS kernel drivers. Finally, our evaluation demonstrates that REDQUEEN is fast, widely applicable and outperforms concurrent approaches by up to three orders of magnitude.},
  comment   = {[talk video](https://youtu.be/9JpanJ29r_U)

}{ Problem

1. magic numbers;
2. (nested) checksums.


}{ Observation

_Input-to-state-correspondence_: During the execution of a given program, parts of the input often end up directly (i.e., nearly unmodified) in the program state.},
  doi       = {10.14722/ndss.2019.23371},
  file      = {:NDSS19-Redqueen.pdf:PDF},
  url       = {https://www.ndss-symposium.org/ndss-paper/redqueen-fuzzing-with-input-to-state-correspondence/},
}

@InProceedings{274634,
  author     = {Andrea Fioraldi and Daniele Cono D{\textquoteright}Elia and Davide Balzarotti},
  booktitle  = {30th {USENIX} Security Symposium ({USENIX} Security 21)},
  title      = {The Use of Likely Invariants as Feedback for Fuzzers},
  year       = {2021},
  month      = aug,
  pages      = {2829--2846},
  publisher  = {{USENIX} Association},
  comment    = {Idea
The key idea is to __augment edge coverage__ —the most widely-adopted and successful code coverage metric used by fuzzers—with information about **local divergences** from ‘usual’ variable values. 

 Steps
1. Run CGF for 24h to _collect samples_ from saved seeds.
2. Use samples to _learn local likely invariants_. 
3. Prune invariants.
   1. Plain fact with _no information_;
   2. Invariants combine _unrelated variables_;
   3. Whenever different invariants have _overlapping condition_.
4. Divide state space using these invariants. 
5. For each edge, generate a different value for the novelty search for each unique combination of violated invariants. 


 Properties

1. Resulting invariants often capture local properties of the test suite more than static properties of the program.
2. State space partition avoids state explosion. 
3. Each basic block has only a few invariants. Since only a few variables are involved.
4. Not all invariants are equally useful.},
  file       = {:usenixsec21_fioraldi.pdf:PDF},
  groups     = {program analysis, feedback},
  isbn       = {978-1-939133-24-3},
  readstatus = {skimmed},
  url        = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
}

@Article{ERNST200735,
  author   = {Michael D. Ernst and Jeff H. Perkins and Philip J. Guo and Stephen McCamant and Carlos Pacheco and Matthew S. Tschantz and Chen Xiao},
  journal  = {Science of Computer Programming},
  title    = {The Daikon system for dynamic detection of likely invariants},
  year     = {2007},
  issn     = {0167-6423},
  note     = {Special issue on Experimental Software and Toolkits},
  number   = {1},
  pages    = {35-45},
  volume   = {69},
  abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon’s output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
  doi      = {https://doi.org/10.1016/j.scico.2007.01.015},
  groups   = {program analysis},
  keywords = {Daikon, Dynamic analysis, Dynamic invariant detection, Inductive logic programming, Inference, Invariant, Likely invariant, Program understanding, Specification, Specification mining},
  url      = {https://www.sciencedirect.com/science/article/pii/S016764230700161X},
}

@InProceedings{raid2019-wang-jinghan,
  author    = {Wang, Jinghan and Duan, Yue and Song, Wei and Yin, Heng and Song, Chengyu},
  booktitle = {22nd International Symposium on Research in Attacks, Intrusions and Defenses ($\{$RAID$\}$ 2019)},
  title     = {Be sensitive and collaborative: Analyzing impact of coverage metrics in greybox fuzzing},
  year      = {2019},
  pages     = {1--15},
  comment   = {Proposes __N-Gram Branch Coverage__


}{ Implement

[afl++ ngram instrument](https://github.com/AFLplusplus/AFLplusplus/blob/stable/instrumentation/README.ngram.md)},
  file      = {:raid2019-wang-jinghan.pdf:PDF},
  groups    = {feedback},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:protocol state fuzzing\;0\;1\;0x008080ff\;\;\;;
1 StaticGroup:machine learning\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:program analysis\;0\;1\;0xffff66ff\;\;\;;
1 StaticGroup:feedback\;0\;0\;0x8a8a8aff\;\;papers proposes new feed back\;;
}
