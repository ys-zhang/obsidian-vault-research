---
annotation-target: papers/machine learning/Decision trees- a recent overview.pdf
---



>%%
>```annotation-json
>{"created":"2022-04-26T11:36:23.922Z","updated":"2022-04-26T11:36:23.922Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":830,"end":1040},{"type":"TextQuoteSelector","exact":"Decision trees are sequential models, which logically combine a sequence of simple tests;each test compares a numeric attribute against a threshold value or a nominal attribute againsta set of possible values. ","prefix":"ication algorithms1 Introduction","suffix":"Such symbolic classifiers have a"}]}]}
>```
>%%
>*%%PREFIX%%ication algorithms1 Introduction%%HIGHLIGHT%% ==Decision trees are sequential models, which logically combine a sequence of simple tests;each test compares a numeric attribute against a threshold value or a nominal attribute againsta set of possible values.== %%POSTFIX%%Such symbolic classifiers have a*
>%%LINK%%[[#^kzt78010b9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kzt78010b9



>%%
>```annotation-json
>{"created":"2022-04-26T11:44:57.099Z","updated":"2022-04-26T11:44:57.099Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":2981,"end":3158},{"type":"TextQuoteSelector","exact":"Numerous decision tree algorithms have been developed over the years, e.g. C4.5 (Quinlan1993), CART (Breiman et al. 1984), SPRINT (Shafer et al. 1996), SLIQ (Mehta et al. 1996).","prefix":" decision trees of smaller size.","suffix":"One of the latest studies that c"}]}]}
>```
>%%
>*%%PREFIX%%decision trees of smaller size.%%HIGHLIGHT%% ==Numerous decision tree algorithms have been developed over the years, e.g. C4.5 (Quinlan1993), CART (Breiman et al. 1984), SPRINT (Shafer et al. 1996), SLIQ (Mehta et al. 1996).== %%POSTFIX%%One of the latest studies that c*
>%%LINK%%[[#^otihm11j2u|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^otihm11j2u


>%%
>```annotation-json
>{"created":"2022-04-26T11:46:52.666Z","updated":"2022-04-26T11:46:52.666Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":3419,"end":3604},{"type":"TextQuoteSelector","exact":"Gehrke etal. (2000) proposed Rainforest, a framework for developing fast and scalable algorithms toconstruct decision trees that gracefully adapt to the amount of main memory available.","prefix":"ning data fits in memory, thus, ","suffix":"We have limited our references t"}]}]}
>```
>%%
>*%%PREFIX%%ning data fits in memory, thus,%%HIGHLIGHT%% ==Gehrke etal. (2000) proposed Rainforest, a framework for developing fast and scalable algorithms toconstruct decision trees that gracefully adapt to the amount of main memory available.== %%POSTFIX%%We have limited our references t*
>%%LINK%%[[#^0nv6pn8gj7b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0nv6pn8gj7b




>%%
>```annotation-json
>{"created":"2022-04-26T11:49:50.569Z","updated":"2022-04-26T11:49:50.569Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":4295,"end":4645},{"type":"TextQuoteSelector","exact":"n Sect. 1, we are referred to methodshow the decision trees handle special problems such as imbalance data, very large datasets,ordinal classification, concept drift etc. Section 4 deals with hybrid decision tree techniquessuch fuzzy decision trees. In Sect. 5, ensembles of decision trees are presented. Finally, thelast section concludes this work.","prefix":"asic issues of decision trees. I","suffix":"2 Basic issuesFigure 1 is an exa"}]}]}
>```
>%%
>*%%PREFIX%%asic issues of decision trees. I%%HIGHLIGHT%% ==n Sect. 1, we are referred to methodshow the decision trees handle special problems such as imbalance data, very large datasets,ordinal classification, concept drift etc. Section 4 deals with hybrid decision tree techniquessuch fuzzy decision trees. In Sect. 5, ensembles of decision trees are presented. Finally, thelast section concludes this work.== %%POSTFIX%%2 Basic issuesFigure 1 is an exa*
>%%LINK%%[[#^sjyp7lx54g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sjyp7lx54g


>%%
>```annotation-json
>{"created":"2022-04-26T11:53:30.954Z","updated":"2022-04-26T11:53:30.954Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":5173,"end":6007},{"type":"TextQuoteSelector","exact":"There are two major phases of the DT induction process: the growth phase and the pruningphase. The growth phase involves a recursive partitioning of the training data resulting in aDT such that either each leaf node is associated with a single class or further partitioning ofthe given leaf would result in at least its child nodes being below some specified threshold.The pruning phase aims to generalize the DT that was generated in the growth phase bygenerating a sub-tree that avoids over-fitting to the training data. The actions of the pruningphase are often referred to as post-pruning in contrast to the pre-pruning that occurs duringthe growth phase and which aims to prevent splits that do not meet certain specified threshold(e.g. minimum number of observations for a split search, minimum number of observationsfor a leaf)","prefix":"ing near-optimal decision trees.","suffix":".123Decision trees: a recent ove"}]}]}
>```
>%%
>*%%PREFIX%%ing near-optimal decision trees.%%HIGHLIGHT%% ==There are two major phases of the DT induction process: the growth phase and the pruningphase. The growth phase involves a recursive partitioning of the training data resulting in aDT such that either each leaf node is associated with a single class or further partitioning ofthe given leaf would result in at least its child nodes being below some specified threshold.The pruning phase aims to generalize the DT that was generated in the growth phase bygenerating a sub-tree that avoids over-fitting to the training data. The actions of the pruningphase are often referred to as post-pruning in contrast to the pre-pruning that occurs duringthe growth phase and which aims to prevent splits that do not meet certain specified threshold(e.g. minimum number of observations for a split search, minimum number of observationsfor a leaf)== %%POSTFIX%%.123Decision trees: a recent ove*
>%%LINK%%[[#^05ygegc658y|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^05ygegc658y


>%%
>```annotation-json
>{"created":"2022-04-26T12:09:13.588Z","updated":"2022-04-26T12:09:13.588Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":7470,"end":7480},{"type":"TextQuoteSelector","exact":"features A","prefix":"ing class  else   {    Find the ","suffix":" that maximizes the goodness mea"}]}]}
>```
>%%
>*%%PREFIX%%ing class  else   {    Find the%%HIGHLIGHT%% ==features A== %%POSTFIX%%that maximizes the goodness mea*
>%%LINK%%[[#^rn3w6te23zn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rn3w6te23zn


>%%
>```annotation-json
>{"created":"2022-04-26T12:09:29.374Z","updated":"2022-04-26T12:09:29.374Z","document":{"title":"","link":[{"href":"urn:x-pdf:1eec96d07534b18d14f8e319af4997ae"},{"href":"vault:/papers/machine learning/Decision trees- a recent overview.pdf"}],"documentFingerprint":"1eec96d07534b18d14f8e319af4997ae"},"uri":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","target":[{"source":"vault:/papers/machine learning/Decision trees- a recent overview.pdf","selector":[{"type":"TextPositionSelector","start":7581,"end":7603},{"type":"TextQuoteSelector","exact":"possible value v of A ","prefix":"r the current node     for each ","suffix":"     {       add a new branch be"}]}]}
>```
>%%
>*%%PREFIX%%r the current node     for each%%HIGHLIGHT%% ==possible value v of A== %%POSTFIX%%{       add a new branch be*
>%%LINK%%[[#^0cqkdi2s8lhm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0cqkdi2s8lhm
