---
annotation-target: https://arxiv.org/pdf/1406.1078.pdf
---

>%%
>```annotation-json
>{"created":"2021-09-06T06:15:23.437Z","updated":"2021-09-06T06:15:23.437Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":458,"end":606},{"type":"TextQuoteSelector","exact":"In  this  paper,  we  propose  a  novel  neu-ral network model called RNN Encoder–Decoder  that  consists  of  two  recurrentneural  networks  (RNN)","prefix":"Fellowfind.me@on.the.webAbstract","suffix":".  One  RNN  en-codes a sequence"}]}]}
>```
>%%
>*%%PREFIX%%Fellowfind.me@on.the.webAbstract%%HIGHLIGHT%% ==In  this  paper,  we  propose  a  novel  neu-ral network model called RNN Encoder–Decoder  that  consists  of  two  recurrentneural  networks  (RNN)== %%POSTFIX%%.  One  RNN  en-codes a sequence*
>%%LINK%%[[#^l99jyx7lqd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l99jyx7lqd


>%%
>```annotation-json
>{"created":"2021-09-06T06:19:36.343Z","updated":"2021-09-06T06:19:36.343Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":769,"end":926},{"type":"TextQuoteSelector","exact":"The encoder and de-coder  of  the  proposed  model  are  jointlytrained to maximize the conditional prob-ability of a target sequence given a sourcesequence.","prefix":" another se-quence of symbols.  ","suffix":"   The performance of a statisti"}]}]}
>```
>%%
>*%%PREFIX%%another se-quence of symbols.%%HIGHLIGHT%% ==The encoder and de-coder  of  the  proposed  model  are  jointlytrained to maximize the conditional prob-ability of a target sequence given a sourcesequence.== %%POSTFIX%%The performance of a statisti*
>%%LINK%%[[#^rbahwvox78|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rbahwvox78


>%%
>```annotation-json
>{"created":"2021-09-06T06:20:48.206Z","updated":"2021-09-06T06:20:48.206Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":928,"end":1173},{"type":"TextQuoteSelector","exact":" The performance of a statisti-cal machine translation system is empiri-cally found to improve by using the con-ditional probabilities of phrase pairs com-puted by the RNN Encoder–Decoder as anadditional feature in the existing log-linearmodel. ","prefix":"quence given a sourcesequence.  ","suffix":"   Qualitatively,  we  show  tha"}]}]}
>```
>%%
>*%%PREFIX%%quence given a sourcesequence.%%HIGHLIGHT%% ==The performance of a statisti-cal machine translation system is empiri-cally found to improve by using the con-ditional probabilities of phrase pairs com-puted by the RNN Encoder–Decoder as anadditional feature in the existing log-linearmodel.== %%POSTFIX%%Qualitatively,  we  show  tha*
>%%LINK%%[[#^3bhw4gwk2gx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3bhw4gwk2gx


>%%
>```annotation-json
>{"created":"2021-09-06T06:20:52.694Z","updated":"2021-09-06T06:20:52.694Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":1176,"end":1312},{"type":"TextQuoteSelector","exact":"Qualitatively,  we  show  that  theproposed model learns a semantically andsyntactically meaningful representation oflinguistic phrases.","prefix":"he existing log-linearmodel.    ","suffix":"1  IntroductionDeep neural netwo"}]}]}
>```
>%%
>*%%PREFIX%%he existing log-linearmodel.%%HIGHLIGHT%% ==Qualitatively,  we  show  that  theproposed model learns a semantically andsyntactically meaningful representation oflinguistic phrases.== %%POSTFIX%%1  IntroductionDeep neural netwo*
>%%LINK%%[[#^4kxioc3mzuw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4kxioc3mzuw


>%%
>```annotation-json
>{"created":"2021-09-06T06:27:08.409Z","updated":"2021-09-06T06:27:08.409Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":2311,"end":2497},{"type":"TextQuoteSelector","exact":"The proposed neural network architecture, whichwe will refer to as anRNN Encoder–Decoder, con-sists of two recurrent neural networks (RNN) thatact  as  an  encoder  and  a  decoder  pair","prefix":"onal  phrase-based  SMT  system.","suffix":".   The  en-coder maps a variabl"}]}]}
>```
>%%
>*%%PREFIX%%onal  phrase-based  SMT  system.%%HIGHLIGHT%% ==The proposed neural network architecture, whichwe will refer to as anRNN Encoder–Decoder, con-sists of two recurrent neural networks (RNN) thatact  as  an  encoder  and  a  decoder  pair== %%POSTFIX%%.   The  en-coder maps a variabl*
>%%LINK%%[[#^7c1c5idyg17|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7c1c5idyg17


>%%
>```annotation-json
>{"created":"2021-09-06T06:27:38.164Z","updated":"2021-09-06T06:27:38.164Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":2793,"end":2934},{"type":"TextQuoteSelector","exact":" Additionally,we  propose  to  use  a  rather  sophisticated  hiddenunit in order to improve both the memory capacityand the ease of training","prefix":"quence given a source sequence. ","suffix":".The  proposed  RNN  Encoder–Dec"}]}]}
>```
>%%
>*%%PREFIX%%quence given a source sequence.%%HIGHLIGHT%% ==Additionally,we  propose  to  use  a  rather  sophisticated  hiddenunit in order to improve both the memory capacityand the ease of training== %%POSTFIX%%.The  proposed  RNN  Encoder–Dec*
>%%LINK%%[[#^l2nm909czx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l2nm909czx


>%%
>```annotation-json
>{"created":"2021-09-06T06:29:56.963Z","updated":"2021-09-06T06:29:56.963Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":3608,"end":3846},{"type":"TextQuoteSelector","exact":"The  qualitative  analysis  shows  that  the  RNNEncoder–Decoder  is  better  at  capturing  the  lin-guistic regularities in the phrase table,  indirectlyexplaining  the  quantitative  improvements  in  theoverall translation performance","prefix":" the existing translation model.","suffix":".  The further anal-ysis of the "}]}]}
>```
>%%
>*%%PREFIX%%the existing translation model.%%HIGHLIGHT%% ==The  qualitative  analysis  shows  that  the  RNNEncoder–Decoder  is  better  at  capturing  the  lin-guistic regularities in the phrase table,  indirectlyexplaining  the  quantitative  improvements  in  theoverall translation performance== %%POSTFIX%%.  The further anal-ysis of the*
>%%LINK%%[[#^7zxboedsbc5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7zxboedsbc5


>%%
>```annotation-json
>{"created":"2021-09-06T06:30:53.487Z","updated":"2021-09-06T06:30:53.487Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":4654,"end":4783},{"type":"TextQuoteSelector","exact":"An  RNN  can  learn  a  probability  distributionover  a  sequence  by  being  trained  to  predict  thenext symbol in a sequence","prefix":"chreiter and Schmidhuber, 1997).","suffix":". In that case, the outputat  ea"}]}]}
>```
>%%
>*%%PREFIX%%chreiter and Schmidhuber, 1997).%%HIGHLIGHT%% ==An  RNN  can  learn  a  probability  distributionover  a  sequence  by  being  trained  to  predict  thenext symbol in a sequence== %%POSTFIX%%. In that case, the outputat  ea*
>%%LINK%%[[#^0t6fxr57t6h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0t6fxr57t6h


>%%
>```annotation-json
>{"created":"2021-09-06T06:31:11.054Z","updated":"2021-09-06T06:31:11.054Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":4465,"end":4654},{"type":"TextQuoteSelector","exact":"fmay   be   as   simple   as   an   element-wise   logistic   sigmoid   function   and   as   com-plex   as   a   long   short-term   memory   (LSTM)unit (Hochreiter and Schmidhuber, 1997).","prefix":"near    activation    func-tion.","suffix":"An  RNN  can  learn  a  probabil"}]}]}
>```
>%%
>*%%PREFIX%%near    activation    func-tion.%%HIGHLIGHT%% ==fmay   be   as   simple   as   an   element-wise   logistic   sigmoid   function   and   as   com-plex   as   a   long   short-term   memory   (LSTM)unit (Hochreiter and Schmidhuber, 1997).== %%POSTFIX%%An  RNN  can  learn  a  probabil*
>%%LINK%%[[#^anspat466e7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^anspat466e7


>%%
>```annotation-json
>{"created":"2021-09-06T08:43:58.938Z","updated":"2021-09-06T08:43:58.938Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":6054,"end":6216},{"type":"TextQuoteSelector","exact":"The encoder is an RNN that reads each symbolof an input sequencexsequentially.  As it readseach symbol, the hidden state of the RNN changesaccording  to  Eq.  (1)","prefix":"sequencelengthsTandT′may differ.","suffix":".   After  reading  the  end  of"}]}]}
>```
>%%
>*%%PREFIX%%sequencelengthsTandT′may differ.%%HIGHLIGHT%% ==The encoder is an RNN that reads each symbolof an input sequencexsequentially.  As it readseach symbol, the hidden state of the RNN changesaccording  to  Eq.  (1)== %%POSTFIX%%.   After  reading  the  end  of*
>%%LINK%%[[#^y1q5wn1yxh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^y1q5wn1yxh


>%%
>```annotation-json
>{"created":"2021-09-06T08:46:05.857Z","updated":"2021-09-06T08:46:05.857Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":6371,"end":6522},{"type":"TextQuoteSelector","exact":"he decoder of the proposed model is anotherRNN which is trained togeneratethe output se-quence by predicting the next symbolytgiven thehidden stateh〈t〉","prefix":"rycof the whole input sequence.T","suffix":".  However, unlike the RNN de-sc"}]}]}
>```
>%%
>*%%PREFIX%%rycof the whole input sequence.T%%HIGHLIGHT%% ==he decoder of the proposed model is anotherRNN which is trained togeneratethe output se-quence by predicting the next symbolytgiven thehidden stateh〈t〉== %%POSTFIX%%.  However, unlike the RNN de-sc*
>%%LINK%%[[#^zx0c4vooae|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zx0c4vooae


>%%
>```annotation-json
>{"created":"2021-09-06T08:47:24.746Z","updated":"2021-09-06T08:47:24.746Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":6851,"end":6928},{"type":"TextQuoteSelector","exact":"or given activation functionsfandg(the lattermust produce valid probabilities","prefix":"yt−2,...,y1,c) =g(h〈t〉,yt−1,c).f","suffix":", e.g. with a soft-max).See Fig."}]}]}
>```
>%%
>*%%PREFIX%%yt−2,...,y1,c) =g(h〈t〉,yt−1,c).f%%HIGHLIGHT%% ==or given activation functionsfandg(the lattermust produce valid probabilities== %%POSTFIX%%, e.g. with a soft-max).See Fig.*
>%%LINK%%[[#^gyx1vzf4mpv|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gyx1vzf4mpv


>%%
>```annotation-json
>{"created":"2021-09-06T08:48:39.539Z","updated":"2021-09-06T08:48:39.539Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":7025,"end":7143},{"type":"TextQuoteSelector","exact":"The  two  components  of  the  proposedRNNEncoder–Decoderare jointly trained to maximizethe conditional log-likelihood","prefix":"he pro-posed model architecture.","suffix":"maxθ1NN∑n=1logpθ(yn|xn),(4)where"}]}]}
>```
>%%
>*%%PREFIX%%he pro-posed model architecture.%%HIGHLIGHT%% ==The  two  components  of  the  proposedRNNEncoder–Decoderare jointly trained to maximizethe conditional log-likelihood== %%POSTFIX%%maxθ1NN∑n=1logpθ(yn|xn),(4)where*
>%%LINK%%[[#^zqdrmxh65y|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zqdrmxh65y



>%%
>```annotation-json
>{"created":"2021-09-06T08:50:40.016Z","updated":"2021-09-06T08:50:40.016Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":7840,"end":8021},{"type":"TextQuoteSelector","exact":"n addition to a novel model architecture, we alsopropose a new type of hidden unit (fin Eq. (1))that has been motivated by the LSTM unit but ismuch simpler to compute and implement.","prefix":"Adaptively Remembersand ForgetsI","suffix":"1Fig. 2shows the graphical depic"}]}]}
>```
>%%
>*%%PREFIX%%Adaptively Remembersand ForgetsI%%HIGHLIGHT%% ==n addition to a novel model architecture, we alsopropose a new type of hidden unit (fin Eq. (1))that has been motivated by the LSTM unit but ismuch simpler to compute and implement.== %%POSTFIX%%1Fig. 2shows the graphical depic*
>%%LINK%%[[#^rh0mmxfx9f|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rh0mmxfx9f


>%%
>```annotation-json
>{"created":"2021-09-06T08:54:30.010Z","updated":"2021-09-06T08:54:30.010Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":8231,"end":8264},{"type":"TextQuoteSelector","exact":"σis the logistic sigmoid function","prefix":"j=σ([Wrx]j+[Urh〈t−1〉]j),(5)where","suffix":", and[.]jdenotes thej-th element"}]}]}
>```
>%%
>*%%PREFIX%%j=σ([Wrx]j+[Urh〈t−1〉]j),(5)where%%HIGHLIGHT%% ==σis the logistic sigmoid function== %%POSTFIX%%, and[.]jdenotes thej-th element*
>%%LINK%%[[#^wvi9ssroc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wvi9ssroc


>%%
>```annotation-json
>{"created":"2021-09-06T08:54:48.590Z","updated":"2021-09-06T08:54:48.590Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":8172,"end":8197},{"type":"TextQuoteSelector","exact":"heresetgaterjiscomputed b","prefix":"idden unit is computed. First, t","suffix":"yrj=σ([Wrx]j+[Urh〈t−1〉]j),(5)whe"}]}]}
>```
>%%
>*%%PREFIX%%idden unit is computed. First, t%%HIGHLIGHT%% ==heresetgaterjiscomputed b== %%POSTFIX%%yrj=σ([Wrx]j+[Urh〈t−1〉]j),(5)whe*
>%%LINK%%[[#^3wh78xh6vld|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3wh78xh6vld


>%%
>```annotation-json
>{"created":"2021-09-06T09:03:16.536Z","updated":"2021-09-06T09:03:16.536Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":8644,"end":8774},{"type":"TextQuoteSelector","exact":"when the reset gate is closeto 0, the hidden state is forced to ignore the pre-vious hidden state and reset with the current input","prefix":"−1〉)]j).(8)In this formulation, ","suffix":"1The LSTM unit, which has shown "}]}]}
>```
>%%
>*%%PREFIX%%−1〉)]j).(8)In this formulation,%%HIGHLIGHT%% ==when the reset gate is closeto 0, the hidden state is forced to ignore the pre-vious hidden state and reset with the current input== %%POSTFIX%%1The LSTM unit, which has shown*
>%%LINK%%[[#^wubnghfk8ws|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wubnghfk8ws


>%%
>```annotation-json
>{"created":"2021-09-06T09:07:01.185Z","updated":"2021-09-06T09:07:01.185Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":9596,"end":9715},{"type":"TextQuoteSelector","exact":"the update gate controls howmuch information from the previous hidden statewill carry over to the current hidden state.","prefix":"presentation.On the other hand, ","suffix":"   Thisacts  similarly  to  the "}]}]}
>```
>%%
>*%%PREFIX%%presentation.On the other hand,%%HIGHLIGHT%% ==the update gate controls howmuch information from the previous hidden statewill carry over to the current hidden state.== %%POSTFIX%%Thisacts  similarly  to  the*
>%%LINK%%[[#^h95pgzdky5|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^h95pgzdky5


>%%
>```annotation-json
>{"created":"2021-09-06T09:12:58.092Z","text":"f: French\ne: English","updated":"2021-09-06T09:12:58.092Z","uri":"https://arxiv.org/pdf/1406.1078.pdf","document":{"title":"1406.1078.pdf","link":[{"href":"urn:x-pdf:4d275f377e04504ca83969d31c5fad7d"},{"href":"https://arxiv.org/pdf/1406.1078.pdf"}],"documentFingerprint":"4d275f377e04504ca83969d31c5fad7d"},"target":[{"source":"https://arxiv.org/pdf/1406.1078.pdf","selector":[{"type":"TextPositionSelector","start":10587,"end":10685},{"type":"TextQuoteSelector","exact":"he  goal  of  the  system  (decoder,specifically) is to find a translationfgiven a sourcesentencee","prefix":"ine translationsystem  (SMT),  t","suffix":", which maximizesp(f|e)∝p(e|f)p("}]}]}
>```
>%%
>*%%PREFIX%%ine translationsystem  (SMT),  t%%HIGHLIGHT%% ==he  goal  of  the  system  (decoder,specifically) is to find a translationfgiven a sourcesentencee== %%POSTFIX%%, which maximizesp(f|e)∝p(e|f)p(*
>%%LINK%%[[#^wsdstouvm0r|show annotation]]
>%%COMMENT%%
>f: French
>e: English
>%%TAGS%%
>
^wsdstouvm0r
